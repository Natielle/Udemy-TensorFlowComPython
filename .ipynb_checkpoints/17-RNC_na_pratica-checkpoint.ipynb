{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo é adivinhar qual é o dígito através da imagem. Banco de dados da Mnist. Pode ser encontrado em: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "Nesse exemplo iremos criar nosso próprio estimator, pois nos exemplos anteriores utilizamos o DNNClassifier, LinearClassifier, entre outros.\n",
    "\n",
    "E Vamos utilizar o high level api do tensorflow que já possui algumas funções definidas para realizarmos a convolução, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-de4c8b48dbd5>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Natielle\\anaconda3\\envs\\tensorflow2_py37\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "# Lendo as imagens e fazendo download\n",
    "mnist = input_data.read_data_sets('mnist/', # As imagens ficarão nesse diretório \n",
    "                                  one_hot = False) # Não faz a categorização \"binária\" de múltiplas classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (55000, 784)\n",
      "X_test:  (10000, 784)\n",
      "Y_train:  (55000,)\n",
      "Y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Separando os dados em treino e teste\n",
    "X_train = mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test  = mnist.test.images\n",
    "Y_test  = mnist.test.labels\n",
    "\n",
    "# Analisando as dimensões\n",
    "print(\"X_train: \", X_train.shape)\n",
    "print(\"X_test: \", X_test.shape)\n",
    "print(\"Y_train: \", Y_train.shape)\n",
    "print(\"Y_test: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota-se que as imagens possuem 28x28 de dimensão, logo as imagens possuem 28*28=784 atributos.\n",
    "\n",
    "Aliás, podemos ver abaixo que os dados já estão normalizados pois os valores vem em escala de 0 a 255. Então não vamos precisar normalizar o banco porque isso já está sendo realizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analizando os valores que temos em Y já que deixamos one_hot = false\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos que não está em formato de zeros e uns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analizando os valores que temos em X\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Classe: 1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAOqElEQVR4nO3df+xddX3H8edLVmQiQxDb1QJWXf+YM7EulczQGQ2TYbMFCNGIbLLMpC6RbEa3SNicxGWLcdNtf0yTMojFCeICjIoykc4JJAypVREBFbFioaMy0BbDtLTv/XFPl6/1+73fL/d3+3k+kpt77/ncc+/7e9JXP59zzzn3k6pC0pHvWdMuQNJkGHapEYZdaoRhlxph2KVGGHapEYb9CJTk0iT/Mu06NFsM+2EqyVuSbEvyZJJdSW5Ksn7adS0myeuSfCHJj5LsmHY9LTHsh6Ek7wL+AfgbYAVwKvAR4Oxp1rVEPwauAP5s2oW0xrAfZpIcD7wfeEdVXVdVP66qfVX16aqaN0BJ/jXJf3e96a1Jfm1O24Yk9ybZm+ThJH/aLT8pyY1Jfpjk8SS3JXlW1/bCJNcm+UGS7yb546XWX1VfqqqPAw8OtSH0jBn2w8+rgWOA65/BOjcBa4DlwHbgE3PaLgfeXlXHAS8H/qNb/m5gJ/ACeqOHS4DqAv9p4GvAKuAM4J1JfhsgyfokPxzsT9M4GfbDz/OBx6rq6aWuUFVXVNXeqvoJcCnwim6EALAPeFmSX6qqJ6pq+5zlK4EXdSOH26p3IcWrgBdU1fur6qdV9SBwGfDm7rNur6rnjeQv1UgZ9sPP/wAnJfmFpbw4yVFJPpDkO0n2ADu6ppO6+/OADcD3knwxyau75X8LPADcnOTBJBd3y18EvLAb3v+w68Uvodf7a4YZ9sPPHcD/Aucs8fVvoffF3W8BxwOru+UBqKq7qupsekP8fwM+1S3fW1XvrqqXAL8LvCvJGcD3ge9W1fPm3I6rqg2j+fM0Lob9MFNVPwL+EvinJOckeU6SZUnekOSD86xyHPATeiOC59D7Bh+AJEcnuSDJ8VW1D9gD7O/afifJryTJnOX7gS8Be5K8J8kvdiOHlyd51VLqT/KsJMcAy3pPc0ySowfdHlo6w34YqqoPA+8C/gL4Ab3e9iJ6PfOhrgS+BzwM3Av81yHtvw/s6Ib4fwT8Xrd8DXAL8CS90cRHquo/q2o/vZ5+LfBd4DHgn+mNGkjym0me7FP+a4CngM/SO2T4FHDzUv92DS7+eIXUBnt2qRGGXWqEYZcaYdilRizpxIxRSeK3gdKYVVXmWz5Uz57krCTfTPLAnDOsJM2ggQ+9JTkK+BbwenoXTNwFnF9V9/ZZx55dGrNx9OynAQ9U1YNV9VPgkxwe11NLTRom7Kvonbl10M5u2c9IsrH7RZVtQ3yWpCEN8wXdfEOFnxumV9UmYBM4jJemaZiefSdwypznJwOPDFeOpHEZJux3AWuSvLi7aunNwJbRlCVp1AYexlfV00kuAj4HHAVcUVXfGFllkkZqole9uc8ujd9YTqqRdPgw7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS40w7FIjDLvUCMMuNcKwS42Y6JTNOvxcdtllfdsvuOCCvu3r169fsG379u0D1aTB2LNLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIj7Orrx07dvRtP+aYY/q2r1mzZsE2j7NP1lBhT7ID2AvsB56uqnWjKErS6I2iZ39dVT02gveRNEbus0uNGDbsBdyc5MtJNs73giQbk2xLsm3Iz5I0hGGH8adX1SNJlgOfT3J/Vd069wVVtQnYBJCkhvw8SQMaqmevqke6+93A9cBpoyhK0ugNHPYkxyY57uBj4EzgnlEVJmm0hhnGrwCuT3Lwfa6qqn8fSVWaGQ899NBQ67/1rW9dsO2aa64Z6r31zAwc9qp6EHjFCGuRNEYeepMaYdilRhh2qRGGXWqEYZca4SWuGqt9+/ZNuwR17NmlRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEx9nV17nnnjvU+ldfffWIKtGw7NmlRhh2qRGGXWqEYZcaYdilRhh2qRGGXWpEqiY3SYszwsyetWvX9m2/8847+7bv2bOnb/upp566YNtTTz3Vd10Npqoy33J7dqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGuH17I179rOf3bd92bJlfdsPHDjQt91j6bNj0Z49yRVJdie5Z86yE5N8Psm3u/sTxlumpGEtZRj/MeCsQ5ZdDGytqjXA1u65pBm2aNir6lbg8UMWnw1s7h5vBs4ZcV2SRmzQffYVVbULoKp2JVm+0AuTbAQ2Dvg5kkZk7F/QVdUmYBN4IYw0TYMeens0yUqA7n736EqSNA6Dhn0LcGH3+ELghtGUI2lcFh3GJ7kaeC1wUpKdwPuADwCfSvI24CHgjeMsUuNz3nnnTbsETciiYa+q8xdoOmPEtUgaI0+XlRph2KVGGHapEYZdaoRhlxrhJa6NW7ly5bRL0ITYs0uNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AivZz/CHX300X3bV69ePdT733///UOtr8mxZ5caYdilRhh2qRGGXWqEYZcaYdilRhh2qREeZz/CHXvssX3bTz/99KHe/5ZbbhlqfU3Ooj17kiuS7E5yz5xllyZ5OMlXu9uG8ZYpaVhLGcZ/DDhrnuV/X1Vru9tnR1uWpFFbNOxVdSvw+ARqkTRGw3xBd1GSu7th/gkLvSjJxiTbkmwb4rMkDWnQsH8UeCmwFtgFfGihF1bVpqpaV1XrBvwsSSMwUNir6tGq2l9VB4DLgNNGW5akURso7EnmzvN7LnDPQq+VNBsWPc6e5GrgtcBJSXYC7wNem2QtUMAO4O1jrFFDGPf86zfddNNY31+js2jYq+r8eRZfPoZaJI2Rp8tKjTDsUiMMu9QIwy41wrBLjfAS1yPce9/73qHW/8xnPtO3/Stf+cpQ76/JsWeXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRHmc/wp1xxhlDrf/EE0/0bd+/f/9Q76/JsWeXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRHmc/AqxYsWLBtmXLlvVdN8moy9GMsmeXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRS5my+RTgSuCXgQPApqr6xyQnAtcAq+lN2/ymqup/8bPGYtOmTQu2HX/88X3Xraq+7VddddVANWn2LKVnfxp4d1X9KvAbwDuSvAy4GNhaVWuArd1zSTNq0bBX1a6q2t493gvcB6wCzgY2dy/bDJwzriIlDe8Z7bMnWQ28ErgTWFFVu6D3HwKwfNTFSRqdJZ8bn+S5wLXAO6tqz1LPqU6yEdg4WHmSRmVJPXuSZfSC/omquq5b/GiSlV37SmD3fOtW1aaqWldV60ZRsKTBLBr29Lrwy4H7qurDc5q2ABd2jy8Ebhh9eZJGJYsdekmyHrgN+Dq9Q28Al9Dbb/8UcCrwEPDGqnp8kffq/2Ga18knn9y3/Y477liwbdWqVX3X3bp1a9/2M888s2/7Yv9+NHlVNe8+9qL77FV1O7DQDvpwP0ouaWI8g05qhGGXGmHYpUYYdqkRhl1qhGGXGuFPSR8Gli/vf9nBYsfS+9m8eXPfdo+jHzns2aVGGHapEYZdaoRhlxph2KVGGHapEYZdaoTH2Y9wt99+e9/2LVu2TKgSTZs9u9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjVj0d+NH+mH+brw0dgv9brw9u9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjVg07ElOSfKFJPcl+UaSP+mWX5rk4SRf7W4bxl+upEEtelJNkpXAyqranuQ44MvAOcCbgCer6u+W/GGeVCON3UIn1Sz6SzVVtQvY1T3em+Q+YPApSCRNxTPaZ0+yGnglcGe36KIkdye5IskJC6yzMcm2JNuGqlTSUJZ8bnyS5wJfBP66qq5LsgJ4DCjgr+gN9f9wkfdwGC+N2ULD+CWFPcky4Ebgc1X14XnaVwM3VtXLF3kfwy6N2cAXwiQJcDlw39ygd1/cHXQucM+wRUoan6V8G78euA34OnCgW3wJcD6wlt4wfgfw9u7LvH7vZc8ujdlQw/hRMezS+Hk9u9Q4wy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41YtEfnByxx4DvzXl+UrdsFs1qbbNaF1jboEZZ24sWapjo9ew/9+HJtqpaN7UC+pjV2ma1LrC2QU2qNofxUiMMu9SIaYd905Q/v59ZrW1W6wJrG9REapvqPrukyZl2zy5pQgy71IiphD3JWUm+meSBJBdPo4aFJNmR5OvdNNRTnZ+um0Nvd5J75iw7Mcnnk3y7u593jr0p1TYT03j3mWZ8qttu2tOfT3yfPclRwLeA1wM7gbuA86vq3okWsoAkO4B1VTX1EzCSvAZ4Erjy4NRaST4IPF5VH+j+ozyhqt4zI7VdyjOcxntMtS00zfgfMMVtN8rpzwcxjZ79NOCBqnqwqn4KfBI4ewp1zLyquhV4/JDFZwObu8eb6f1jmbgFapsJVbWrqrZ3j/cCB6cZn+q261PXREwj7KuA7895vpPZmu+9gJuTfDnJxmkXM48VB6fZ6u6XT7meQy06jfckHTLN+Mxsu0GmPx/WNMI+39Q0s3T87/Sq+nXgDcA7uuGqluajwEvpzQG4C/jQNIvpphm/FnhnVe2ZZi1zzVPXRLbbNMK+EzhlzvOTgUemUMe8quqR7n43cD293Y5Z8ujBGXS7+91Truf/VdWjVbW/qg4AlzHFbddNM34t8Imquq5bPPVtN19dk9pu0wj7XcCaJC9OcjTwZmDLFOr4OUmO7b44IcmxwJnM3lTUW4ALu8cXAjdMsZafMSvTeC80zThT3nZTn/68qiZ+AzbQ+0b+O8CfT6OGBep6CfC17vaNadcGXE1vWLeP3ojobcDzga3At7v7E2eoto/Tm9r7bnrBWjml2tbT2zW8G/hqd9sw7W3Xp66JbDdPl5Ua4Rl0UiMMu9QIwy41wrBLjTDsUiMMu9QIwy414v8ATYue0Xhc1pUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agora vamos observar de fato alguma imagem carregada\n",
    "%matplotlib inline\n",
    "indice_imagem = 4\n",
    "plt.imshow(X_train[indice_imagem].reshape((28,28)), cmap = 'gray') # em esscala de cinza\n",
    "plt.title('Classe: ' + str(Y_train[indice_imagem]))\n",
    "\n",
    "# Obs: temos que fazer o reshape ali acima porque os atributos da imagem (em X) \n",
    "# está como se fosse um vetor. E para plotarmos uma imagem, precisamos deixar no formato de matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que a gente faça o modelo do classificador, precisamos passar uma função para o estimator. Pois nesse exemplo, vamos fazer a nossa própria\n",
    "\n",
    "Com isso vamos criar uma função que tem alguns parâmetros obrigatórios.\n",
    "Os parâmetros são: \n",
    "- features (representa os atributos)\n",
    "- labels (representa as classes)\n",
    "\n",
    "Há também parâmetros opcionais, como o: \n",
    "- mode.\n",
    "\n",
    "O mode indica como está sendo rodada a função. Se é para treinar, testar ou prever. **É importante notar que o mode é passado implicitamente. Ou seja, dependendo da função que você utilizar ele vai passar um valor específico.**\n",
    "\n",
    "\n",
    "*Exemplo:*\n",
    "Se utilizar a função train, mode será o modo TRAIN(treino) e não EVAL(teste). Nem PREDICT(predição).\n",
    "\n",
    "\n",
    "E Nota-se que os nomes precisam ser exatamente assim.\n",
    "\n",
    "E a função deve retornar um objeto do tipo EstimatorSpec! Porém, para conseguirmos construir um objeto desse tipo precisamos definir todo o processo de convolução, pooling e flattening.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aqui vamos fazer apenas um exemplo para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Então vamos fazer a função que utilizaremos com o estimator\n",
    "def cria_rede_train(features, labels, mode):\n",
    "    # Precisamos definir a camada de entrada dos dados (nesse caso é a imagem)\n",
    "    # Estrutura do reshape: batch_size, largura, altura, canais -> [-1, 28, 28, 1]\n",
    "    entrada = tf.reshape(features['X'], [-1, 28, 28, 1])\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    #       APLICAÇÃO A CONVOLUÇÃO E POOLING\n",
    "    # ----------------------------------------------------\n",
    "    # Nessa implementação vamos fazer duas camadas de convolução. \n",
    "    # E para cada camada de convolução teremos uma camada de pooling. Então faremos em pares.\n",
    "    \n",
    "    # --------------------------\n",
    "    #       CAMADA 1\n",
    "    # --------------------------\n",
    "    # Estamos fazendo a 1° camada de convolução\n",
    "    # recebe [batch_size, 28, 28, 1] -> [batch_size, largura, altura, canais]\n",
    "    # retorna [batch_size, 28, 28, 32] -> [batch_size, largura, altura, canais]\n",
    "    convolucao1 = tf.layers.conv2d(inputs = entrada, \n",
    "               filters = 32,   # qtde de feature maps que vamos gerar para a convolução\n",
    "               kernel_size=[5,5], # dimensões do filtro de características (kernel)\n",
    "               activation = tf.nn.relu, # vamos aplicar a função RELU em cada feature maps\n",
    "               padding = 'same') # preserva a altura e largura da imagem após as transformações\n",
    "    \n",
    "    # recebe [batch_size, 28, 28, 32]\n",
    "    # retorna [batch_size, 14, 14, 32] \n",
    "    # podemos ver que a dimensionalidade da imagem foi diminuida pela metade, pois pool_size=(2,2)\n",
    "    pooling1 = tf.layers.max_pooling2d(inputs = convolucao1, # camada anterior\n",
    "                   pool_size = [2,2], # dimensões do 'kernel' para aplicar o pooling no feature map\n",
    "                   strides = 2) # significa que o 'kernel' vai ser aplicado a cada 2 pixels\n",
    "    \n",
    "    # --------------------------\n",
    "    #       CAMADA 2\n",
    "    # --------------------------\n",
    "    # Estamos fazendo a 1° camada de convolução\n",
    "    # recebe [batch_size, 14, 14, 32] -> [batch_size, largura, altura, canais]\n",
    "    # retorna [batch_size, 14, 14, 64] -> [batch_size, largura, altura, canais]\n",
    "    convolucao2 = tf.layers.conv2d(inputs = pooling1, \n",
    "               filters = 64,   # qtde de feature maps que vamos gerar para a convolução\n",
    "               kernel_size=[5,5], # dimensões do filtro de características (kernel)\n",
    "               activation = tf.nn.relu, # vamos aplicar a função RELU em cada feature maps\n",
    "               padding = 'same') # preserva a altura e largura da imagem após as transformações\n",
    "    \n",
    "    # recebe [batch_size, 14, 14, 64]\n",
    "    # retorna [batch_size, 7, 7, 64] \n",
    "    # podemos ver que a dimensionalidade da imagem foi diminuida pela metade, pois pool_size=(2,2)\n",
    "    pooling2 = tf.layers.max_pooling2d(inputs = convolucao2, # camada anterior\n",
    "                   pool_size = [2,2], # dimensões do 'kernel' para aplicar o pooling no feature map\n",
    "                   strides = 2) # significa que o 'kernel' vai ser aplicado a cada 2 pixels\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    #       APLICAÇÃO DO FLATTENING\n",
    "    # ----------------------------------------------------\n",
    "    # recebe [batch_size, 7, 7, 64]\n",
    "    # retornar [batch_size, 3136] -> [-1, 7 * 7 * 64]\n",
    "    flattening = tf.reshape(pooling2, [-1, 7 * 7 * 64]) # [batch_size, tamanho do vetor flattening]\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    #       FAZENDO A REDE NEURAL DENSA\n",
    "    # ----------------------------------------------------\n",
    "    # Nossa rede neural densa terá:\n",
    "    # neurônios de entrada: 3136, que são os vetores flattened\n",
    "    # 1 camada oculta de 1024 neurônios \n",
    "    # e 10 neurônios de saída, onde cada um representa a classe de número que queremos advinhar\n",
    "    # recebe [batch_size, 3136]\n",
    "    # retornar [batch_size, 1024]\n",
    "    densa = tf.layers.dense(inputs = flattening, # neurônios de entrada\n",
    "                            units = 1024, # neurônios da camada oculta\n",
    "                            activation = tf.nn.relu) # função de ativação da camada\n",
    "    \n",
    "    # De modo a reduzir um pouco o overfitting da rede, que é quando a rede se adapta demais\n",
    "    # ao dados. Vamos zerar uma porcentagem dos neurônios de entrada\n",
    "    dropout = tf.layers.dropout(inputs = densa, # rede neural densa\n",
    "                rate = 0.2) # taxa de entradas que será zeradas. 20%.\n",
    "             \n",
    "    # Então vamos fazer a camada de saída da rede agora\n",
    "    # recebe [batch_size, 1024]\n",
    "    # retornar [batch_size, 10]\n",
    "    saida = tf.layers.dense(inputs = dropout, # neurônios de entrada com uma taxa de zeros\n",
    "                            units = 10) # qtde de neurônios de saída\n",
    "    \n",
    "    # Agora podemos calcular o erro\n",
    "    # Notar que antes usamos a tf.nn.softmax_cross_entropy_with_logits_v2 para calcular o erro,\n",
    "    # pois tinhamos o one_hot = True. Como temos o one_hot aqui = False, usamos outra função\n",
    "    erro = tf.losses.sparse_softmax_cross_entropy(labels = labels,\n",
    "                                                  logits = saida) # probabilidades de cada classe\n",
    "    \n",
    "    otimizador = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "    treinamento = otimizador.minimize(erro, global_step = tf.train.get_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode = mode, loss = erro, train_op=treinamento)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmp53hy9et4\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Natielle\\\\AppData\\\\Local\\\\Temp\\\\tmp53hy9et4', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A62F686DA0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmp53hy9et4\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3071263, step = 1\n",
      "INFO:tensorflow:global_step/sec: 3.28688\n",
      "INFO:tensorflow:loss = 0.11310303, step = 101 (30.426 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmp53hy9et4\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.11793225.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1a62ef8aeb8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para ficar no formato certo, temos que converter Y para int32 ou int64\n",
    "Y_train = np.asarray(Y_train, dtype = np.int32)\n",
    "Y_test = np.asarray(Y_test, dtype = np.int32)\n",
    "\n",
    "# Criando o modelo do classificador\n",
    "classificador = tf.estimator.Estimator(model_fn = cria_rede_train)\n",
    "\n",
    "# Criando a função para receber os dados\n",
    "funcao_treinamento = tf.estimator.inputs.numpy_input_fn(x = {'X': X_train}, y = Y_train,\n",
    "                                                   batch_size = 128,  # pega em lotes de 128\n",
    "                                                   num_epochs = None, # qtde de vezes que rodará\n",
    "                                                   shuffle = True) # pega ordem aleatória\n",
    "\n",
    "# Treinando o modelo\n",
    "classificador.train(input_fn=funcao_treinamento, steps = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aqui vamos fazer um exemplo para treinamento, teste e predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos melhorar a função para executar nos diferentes modos possíveis\n",
    "def cria_rede(features, labels, mode):\n",
    "    # Precisamos definir a camada de entrada dos dados (nesse caso é a imagem)\n",
    "    # Estrutura do reshape: batch_size, largura, altura, canais -> [-1, 28, 28, 1]\n",
    "    entrada = tf.reshape(features['X'], [-1, 28, 28, 1])\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    #       APLICAÇÃO A CONVOLUÇÃO E POOLING\n",
    "    # ----------------------------------------------------\n",
    "    # Nessa implementação vamos fazer duas camadas de convolução. \n",
    "    # E para cada camada de convolução teremos uma camada de pooling. Então faremos em pares.\n",
    "    \n",
    "    # --------------------------\n",
    "    #       CAMADA 1\n",
    "    # --------------------------\n",
    "    # Estamos fazendo a 1° camada de convolução\n",
    "    # recebe [batch_size, 28, 28, 1] -> [batch_size, largura, altura, canais]\n",
    "    # retorna [batch_size, 28, 28, 32] -> [batch_size, largura, altura, canais]\n",
    "    convolucao1 = tf.layers.conv2d(inputs = entrada, \n",
    "               filters = 32,   # qtde de feature maps que vamos gerar para a convolução\n",
    "               kernel_size=[5,5], # dimensões do filtro de características (kernel)\n",
    "               activation = tf.nn.relu, # vamos aplicar a função RELU em cada feature maps\n",
    "               padding = 'same') # preserva a altura e largura da imagem após as transformações\n",
    "    \n",
    "    # recebe [batch_size, 28, 28, 32]\n",
    "    # retorna [batch_size, 14, 14, 32] \n",
    "    # podemos ver que a dimensionalidade da imagem foi diminuida pela metade, pois pool_size=(2,2)\n",
    "    pooling1 = tf.layers.max_pooling2d(inputs = convolucao1, # camada anterior\n",
    "                   pool_size = [2,2], # dimensões do 'kernel' para aplicar o pooling no feature map\n",
    "                   strides = 2) # significa que o 'kernel' vai ser aplicado a cada 2 pixels\n",
    "    \n",
    "    # --------------------------\n",
    "    #       CAMADA 2\n",
    "    # --------------------------\n",
    "    # Estamos fazendo a 1° camada de convolução\n",
    "    # recebe [batch_size, 14, 14, 32] -> [batch_size, largura, altura, canais]\n",
    "    # retorna [batch_size, 14, 14, 64] -> [batch_size, largura, altura, canais]\n",
    "    convolucao2 = tf.layers.conv2d(inputs = pooling1, \n",
    "               filters = 64,   # qtde de feature maps que vamos gerar para a convolução\n",
    "               kernel_size=[5,5], # dimensões do filtro de características (kernel)\n",
    "               activation = tf.nn.relu, # vamos aplicar a função RELU em cada feature maps\n",
    "               padding = 'same') # preserva a altura e largura da imagem após as transformações\n",
    "    \n",
    "    # recebe [batch_size, 14, 14, 64]\n",
    "    # retorna [batch_size, 7, 7, 64] \n",
    "    # podemos ver que a dimensionalidade da imagem foi diminuida pela metade, pois pool_size=(2,2)\n",
    "    pooling2 = tf.layers.max_pooling2d(inputs = convolucao2, # camada anterior\n",
    "                   pool_size = [2,2], # dimensões do 'kernel' para aplicar o pooling no feature map\n",
    "                   strides = 2) # significa que o 'kernel' vai ser aplicado a cada 2 pixels\n",
    "    \n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    #       APLICAÇÃO DO FLATTENING\n",
    "    # ----------------------------------------------------\n",
    "    # recebe [batch_size, 7, 7, 64]\n",
    "    # retornar [batch_size, 3136] -> [-1, 7 * 7 * 64]\n",
    "    flattening = tf.reshape(pooling2, [-1, 7 * 7 * 64]) # [batch_size, tamanho do vetor flattening]\n",
    "    \n",
    "    # ----------------------------------------------------\n",
    "    #       FAZENDO A REDE NEURAL DENSA\n",
    "    # ----------------------------------------------------\n",
    "    # Nossa rede neural densa terá:\n",
    "    # neurônios de entrada: 3136, que são os vetores flattened\n",
    "    # 1 camada oculta de 1024 neurônios \n",
    "    # e 10 neurônios de saída, onde cada um representa a classe de número que queremos advinhar\n",
    "    # recebe [batch_size, 3136]\n",
    "    # retornar [batch_size, 1024]\n",
    "    densa = tf.layers.dense(inputs = flattening, # neurônios de entrada\n",
    "                            units = 1024, # neurônios da camada oculta\n",
    "                            activation = tf.nn.relu) # função de ativação da camada\n",
    "    \n",
    "    \n",
    "    # De modo a reduzir um pouco o overfitting da rede, que é quando a rede se adapta demais\n",
    "    # ao dados. Vamos zerar uma porcentagem dos neurônios de entrada.\n",
    "    # Portanto, o dropout só poderá ser executado se estivermos treinando a rede.\n",
    "    dropout = tf.layers.dropout(inputs = densa, # rede neural densa\n",
    "                rate = 0.2, # taxa de entradas que será zeradas. 20%.\n",
    "                training = mode == tf.estimator.ModeKeys.TRAIN) # só será executado se estiver no modo de treino\n",
    "    \n",
    "    # Então vamos fazer a camada de saída da rede agora\n",
    "    # recebe [batch_size, 1024]\n",
    "    # retornar [batch_size, 10]\n",
    "    saida = tf.layers.dense(inputs = dropout, # neurônios de entrada com uma taxa de zeros\n",
    "                            units = 10) # qtde de neurônios de saída\n",
    "    \n",
    "    # Quando vamos realizar os testes, precisamos encontrar qual é a maior probabilidade que foi obtida\n",
    "    previsoes = tf.argmax(saida, axis = 1) # argmax retorna o indice que tem maior probabilidade\n",
    "    \n",
    "    # Caso estejamos em modo de predição, não precisamos calcular os erros\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions = previsoes)\n",
    "    \n",
    "    # Agora podemos calcular o erro\n",
    "    # Notar que antes usamos a tf.nn.softmax_cross_entropy_with_logits_v2 para calcular o erro,\n",
    "    # pois tinhamos o one_hot = True. Como temos o one_hot aqui = False, usamos outra função\n",
    "    erro = tf.losses.sparse_softmax_cross_entropy(labels = labels,\n",
    "                                                  logits = saida) # probabilidades de cada classe\n",
    "    \n",
    "    \n",
    "    # Quando estamos treinando apenas, pois não precisamos realizar as otimizações nos outros modos\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        otimizador = tf.train.AdamOptimizer(learning_rate = 0.001)\n",
    "        treinamento = otimizador.minimize(erro, global_step = tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, loss = erro, train_op=treinamento)\n",
    "    \n",
    "    # Quando estamos realizando o teste\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        eval_metrics_ops = {'accuracy': tf.metrics.accuracy(labels = labels, predictions = previsoes)}\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, loss = erro, eval_metric_ops = eval_metrics_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmpz3qhiefd\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\Natielle\\\\AppData\\\\Local\\\\Temp\\\\tmpz3qhiefd', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001A62F1EC908>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmpz3qhiefd\\model.ckpt.\n",
      "INFO:tensorflow:loss = 2.299608, step = 1\n",
      "INFO:tensorflow:global_step/sec: 2.50537\n",
      "INFO:tensorflow:loss = 0.14344609, step = 101 (39.915 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmpz3qhiefd\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.032005433.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1a62f73e358>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aqui vamos realizar o treino com a função mais genérica\n",
    "\n",
    "# Criando o modelo do classificador\n",
    "classificador = tf.estimator.Estimator(model_fn = cria_rede)\n",
    "\n",
    "# Criando a função para receber os dados\n",
    "funcao_treinamento = tf.estimator.inputs.numpy_input_fn(x = {'X': X_train}, y = Y_train,\n",
    "                                                   batch_size = 128,  # pega em lotes de 128\n",
    "                                                   num_epochs = None, # qtde de vezes que rodará\n",
    "                                                   shuffle = True) # pega ordem aleatória\n",
    "\n",
    "# Treinando o modelo\n",
    "classificador.train(input_fn=funcao_treinamento, steps = 200) # nesse caso, o valor de mode será TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-05-09T23:17:40Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmpz3qhiefd\\model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-05-09-23:17:50\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.9812, global_step = 200, loss = 0.059330445\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmpz3qhiefd\\model.ckpt-200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9812, 'loss': 0.059330445, 'global_step': 200}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aqui vamos criar a parte de teste\n",
    "funcao_teste = tf.estimator.inputs.numpy_input_fn(x = {'X': X_test}, y = Y_test, \n",
    "                                                  num_epochs = 1, # vamos fazer o teste apenas 1 vez\n",
    "                                                  shuffle = False) # pega em ordem\n",
    "resultados = classificador.evaluate(input_fn=funcao_teste) # nesse caso, o valor de mode será EVAL\n",
    "resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensões originais da imagem:  (784,)\n",
      "Dimensões corretas para o algoritmo:  (1, 784)\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Natielle\\AppData\\Local\\Temp\\tmpz3qhiefd\\model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "\n",
      "\n",
      "Previsão obtida da rede:  [2]\n"
     ]
    }
   ],
   "source": [
    "# Aqui vamos fazer a parte de predição\n",
    "X_imagem_teste = X_test[1] # obtendo uma imagem qualquer do conjunto de teste\n",
    "\n",
    "print(\"Dimensões originais da imagem: \", X_imagem_teste.shape)\n",
    "X_imagem_teste = X_imagem_teste.reshape(1,-1) \n",
    "print(\"Dimensões corretas para o algoritmo: \", X_imagem_teste.shape) # representa uma imagem com 784 atributos\n",
    "\n",
    "\n",
    "funcao_previsao = tf.estimator.inputs.numpy_input_fn(x = {'X': X_imagem_teste},\n",
    "                                                     shuffle = False) # mantem a ordem dos registros\n",
    "pred = list(classificador.predict(input_fn = funcao_previsao))\n",
    "print(\"\\n\\nPrevisão obtida da rede: \", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Classe prevista: 2')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# para verificarmos se o algoritmo acertou a classe da imagem X_test[1], vamos plotá-la\n",
    "plt.imshow(X_imagem_teste.reshape((28, 28)), cmap = 'gray')\n",
    "plt.title('Classe prevista: ' + str(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo, podemos ver que a CNN conseguiu acertar qual era o número."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2_py37",
   "language": "python",
   "name": "tensorflow2_py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
